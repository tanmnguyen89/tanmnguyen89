<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><br /></div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<br>
<div class="menu-item"><a href="workshops.html">Workshops/&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<h1 style="color:steelblue;">Tan Minh Nguyen</h1>
<table class="imgtable"><tr><td>
<img src="tan_profile.jpeg" alt="Coming Soon!" width="320px" height="320px" />&nbsp;</td>
<td align="left"><p>Assistant Professor <br />
<a href="https://www.math.nus.edu.sg/">Department of Mathematics</a> <br />
<a href="https://nus.edu.sg/">National University of Singapore</a> <br /></p>
<p>Email: <a href="mailto:tanmn@nus.edu.sg">tanmn@nus.edu.sg</a> <br />
Office: 10 Lower Kent Ridge Road, S17-08-20 (Building S17, Floor 8, Room 20), Singapore 119076 </p>
<p><b>Education</b></p>
Ph.D. in Electrical and Computer Engineering, Rice University, 2020 <br />
M.S. in Electrical and Computer Engineering, Rice University, 2016 <br />
B.S. in Electrical and Computer Engineering, Rice University, 2014 <br />
<p> 
<a href="Tan_Nguyen_CV.pdf"><font color=blue size=+0.3> CV</font></a>, <a href="https://github.com/minhtannguyen/"><font color=blue size=+0.3> Github</font></a>, <a href="https://scholar.google.com/citations?user=OizOh88AAAAJ&hl=en"><font color=blue size=+0.3> Google Scholar</font></a>
</p>
</td></tr></table>
<h2 style="color:steelblue;">Brief Biography</h2>
<hr>
<p>I am currently an Assistant Professor of Mathematics at the National University of Singapore (NUS). Before joining NUS, I was a postdoctoral scholar in the Department of Mathematics at the University of California, Los Angeles, working with <a href="https://www.math.ucla.edu/~sjo/">Dr. Stanley J. Osher</a>. I obtained my Ph.D. in Machine Learning from Rice University, where I was advised by <a href="https://richb.rice.edu/">Dr. Richard G. Baraniuk</a>. I gave an invited talk in the Deep Learning Theory Workshop at NeurIPS 2018 and organized the 1st Workshop on Integration of Deep Neural Models and Differential Equations at ICLR 2020. I also had two awesome long internships with Amazon AI and NVIDIA Research, during which I worked with <a href="http://tensorlab.cms.caltech.edu/users/anima/">Dr. Anima Anandkumar</a>. I am the recipient of the prestigious <a href="https://cra.org/ccc/leadership-development/cifellows/">Computing Innovation Postdoctoral Fellowship (CIFellows)</a> from the Computing Research Association (CRA), the <a href="https://www.nsfgrfp.org/">NSF Graduate Research Fellowship</a>, and the <a href="http://www.igert.org/projects/301.html#:~:text=trainees%20are%E2%80%A6%20more%20%C2%BB-,This%20Integrative%20Graduate%20Education%20and%20Research%20Traineeship%20(IGERT)%20award%20provides,%2C%20mechanical%20engineering%2C%20and%20bioengineering.">IGERT Neuroengineering Traineeship</a>. I received my M.S. and B.S. in Electrical and Computer Engineering from Rice in May 2018 and May 2014, respectively.</p>
<p>I am also writing about simple ideas and principled approaches that lead to working machine learning algorithms on my blog, <a href="http://almostconvergent.blogs.rice.edu/">Almost Convergent</a>.</p>
<h2 style="color:steelblue;">Research Interests</h2>
<hr>
<p>My research focuses on the interplay of the interpretability, robustness, and efficiency of machine learning models from three principled approaches:</p>
<ul>
<li><p>Optimization (primal-dual frameworks for deep learning models, momentum-based neural networks, fast multipole transformers)</p>
</li>
<li><p>Differential equations (Nesterov neural ordinary differential equations, graph neural diffusion)</p>
</li>
<li><p>Statistical modeling (mixture and nonparametric kernel regression frameworks for transformers, deep generative models) </p>
</li>
</ul>
<h2 style="color:steelblue;"> Publications</h2>
<hr style="margin-bottom:0cm;">
<h3 style="color:steelblue;"> Journal Publications</h3>
<ul>
<li><p><a href="https://arxiv.org/pdf/1811.02657.pdf"><font color=blue size=+0.3> A Bayesian Perspective of Convolutional Neural Networks through a Deconvolutional Generative Model</font></a>. <i>Journal of Machine Learning Research, 2023</i>.
<br />
<b>Tan M. Nguyen</b><font color=red size=-0.3><b>*</b></font>, Nhat Ho<font color=red size=-0.3><b>*</b></font>, Ankit B. Patel, Anima Anandkumar, Michael I. Jordan, Richard G. Baraniuk.</p>
</li>
</ul>
<ul>
<li><p><a href="Scheduled_Restart_SGD.pdf"><font color=blue size=+0.3> Scheduled Restart Momentum for Accelerated Stochastic Gradient Descent</font></a>. <i>SIAM Journal on Imaging Sciences, 2022</i>. 
<br />
Bao Wang<font color=red size=-0.3><b>*</b></font>, <b>Tan M. Nguyen</b><font color=red size=-0.3><b>*</b></font>, Andrea L. Bertozzi, Richard G. Baraniuk, Stanley J. Osher.</p>
</li>
</ul>
<ul>
<li><p><a href="How_Does_Momentum_Benefit_DNN_Design.pdf"><font color=blue size=+0.3> How Does Momentum Benefit Deep Neural Networks Architecture Design? A Few Case Studies</font></a>. <i> Research in the Mathematical Sciences, 2022</i>.
<br />
Bao Wang, Hedi Xia, <b>Tan M. Nguyen</b>, Stanley J. Osher.  </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1907.04523.pdf"><font color=blue size=+0.3> Dual Dynamic Inference: Enabling More Eﬀicient, Adaptive, and Controllable Deep Inference</font></a>. <i>IEEE Journal of Selected Topics in Signal Processing, 2020</i>. 
<br />
  Yue Wang, Jianghao Shen, Ting-Kuei Hu, Pengfei Xu, <b>Tan M. Nguyen</b>, Richard G. Baraniuk, Zhangyang Wang, Yingyan Lin.</p>
</li>
</ul>
<h3 style="color:steelblue;"> Conference Publications</h3>
<ul>
<li><p><a href="Revisiting_over_smoothing_and_over_squashing.pdf"><font color=blue size=+0.3> Revisiting Over-smoothing and Over-squashing using Ollivier-Ricci Curvature</font></a>. <i>International Conference on Machine Learning (ICML), 2023</i>.
<br />
Khang Nguyen, Hieu Nong, Vinh Nguyen, Nhat Ho, Stanley J. Osher, <b>Tan M. Nguyen</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="neural_collapse_in_deep_linear.pdf"><font color=blue size=+0.3> Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data</font></a>. <i>International Conference on Machine Learning (ICML), 2023</i>.
<br />
Hien Dang<font color=red size=-0.3><b>*</b></font>, Tho Tran Huu<font color=red size=-0.3><b>*</b></font>, Stanley J. Osher, Hung Tran-The, Nhat Ho<font color=red size=-0.3><b>**</b></font>, <b>Tan M. Nguyen</b><font color=red size=-0.3><b>**</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="Primal_Dual_Transformers.pdf"><font color=blue size=+0.3> A Primal-Dual Framework for Transformers and Neural Networks</font></a>. <i>International Conference on Learning Representations (ICLR) (notable-top-25%), 2023</i>.
<br />
<b>Tan M. Nguyen</b><font color=red size=-0.3><b>*</b></font>, Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Nhat Ho, Andrea L. Bertozzi, Richard G. Baraniuk, Stanley J. Osher.</p>
</li>
</ul>
<ul>
<li><p><a href="FourierFormer.pdf"><font color=blue size=+0.3> FourierFormer: Transformer Meets Generalized Fourier Integral Theorem</font></a>. <i>Conference on Neural Information Processing Systems (NeurIPS), 2022.</i>
<br />
<b>Tan M. Nguyen</b><font color=red size=-0.3><b>*</b></font>, Minh Pham<font color=red size=-0.3><b>*</b></font>, Tam Nguyen, Khai Nguyen, Stanley J. Osher, Nhat Ho. </p>
</li>
</ul>
<ul>
<li><p><a href="FiSHformer.pdf"><font color=blue size=+0.3> Improving Transformer with an Admixture of Attention Heads</font></a>. <i>Conference on Neural Information Processing Systems (NeurIPS), 2022.</i>
<br />
<b>Tan M. Nguyen</b><font color=red size=-0.3><b>*</b></font>, Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Hai Do, Khai Nguyen, Vishwanath Saragadam, Minh Pham, Khuong Nguyen, Nhat Ho<font color=red size=-0.3><b>**</b></font>,  Stanley J. Osher<font color=red size=-0.3><b>**</b></font>. </p>
</li>
</ul>
<ul>
<li><p><a href="NesterovNODE.pdf"<font color=blue size=+0.3> Improving Neural Ordinary Differential Equations with Nesterov’s Accelerated Gradient Method</font></a>. <i>Conference on Neural Information Processing Systems (NeurIPS), 2022</i>.
<br />
Nghia Nguyen<font color=red size=-0.3><b>*</b></font>, <b>Tan M. Nguyen</b><font color=red size=-0.3><b>*</b></font>, Huyen Vo, Stanley J. Osher, Thieu Vo.</p>
</li>
</ul>
<ul>
<li><p><a href="MGK.pdf"><font color=blue size=+0.3> Improving Transformers with Probabilistic Attention Keys</font></a>. <i> International Conference on Machine Learning (ICML), 2022</i>.
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, <b>Tan M. Nguyen</b><font color=red size=-0.3><b>*</b></font>, Dung Le, Khuong Nguyen, Anh Tran, Richard G. Baraniuk, Nhat Ho<font color=red size=-0.3><b>**</b></font>,  Stanley J. Osher<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/pdf?id=EMxu-dzvJk"><font color=blue size=+0.3> GRAND++: Graph Neural Diffusion with a Source Term</font></a>. <i>International Conference on Learning Representations (ICLR), 2022</i>.
<br />
Matthew Thorpe<font color=red size=-0.3><b>*</b></font>, <b>Tan M. Nguyen</b><font color=red size=-0.3><b>*</b></font>, Hedi Xia<font color=red size=-0.3><b>*</b></font>, Thomas Strohmer, Andrea Bertozzi, Stanley J. Osher, Bao Wang.</p>
</li>
</ul>
<ul>
<li><p><a href="Momentum_Transformer.pdf"><font color=blue size=+0.3> Momentum Transformer: Closing the Performance Gap Between Self-attention and Its Linearization</font></a>. <i> Mathematical and Scientific Machine Learning (MSML), 2022</i>. 
<br />
<b>Tan M. Nguyen</b>, Richard G. Baraniuk, Mike Kirby, Stanley J. Osher, Bao Wang.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1912.03978.pdf"><font color=blue size=+0.3> InfoCNF: An Efficient Conditional Continuous Normalizing Flow with Adaptive Solvers</font></a>. <i>Asilomar Conference, 2022</i>.
<br />
<b>Tan M. Nguyen</b>, Animesh Garg, Richard G Baraniuk, Anima Anandkumar. </p>
</li>
</ul>
 <ul>
<li><p><a href="FiAK.pdf"<font color=blue size=+0.3> Probabilistic Framework for Pruning Transformers via a Finite Admixture of Keys</font></a>. <i> International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2023</i>. 
<br />
<b>Tan M. Nguyen</b><font color=red size=-0.3><b>*</b></font>, Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Long Bui<font color=red size=-0.3><b>*</b></font>, Hai Do, Dung Le, Hung Tran-The, Khuong Nguyen, Richard G. Baraniuk, Nhat Ho<font color=red size=-0.3><b>**</b></font>,  Stanley J. Osher<font color=red size=-0.3><b>**</b></font>.
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2108.02347.pdf"><font color=blue size=+0.3> FMMformer: Eﬀicient and Flexible Transformer via Decomposed Near-field and Far-field Attention</font></a>. <i>Conference on Neural Information Processing Systems (NeurIPS), 2021</i>.
<br />
<b>Tan M. Nguyen</b>, Vai Suliafu, Stanley J. Osher, Long Chen, Bao Wang.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2110.04840.pdf"><font color=blue size=+0.3> Heavy Ball Neural Ordinary Differential Equations</font></a>. <i> Conference on Neural Information Processing Systems (NeurIPS), 2021</i>. 
<br />
Hedi Xia, Vai Suliafu, Hangjie Ji, <b>Tan M. Nguyen</b>, Andrea Bertozzi, Stanley J. Osher, Bao Wang. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.06919.pdf"<font color=blue size=+0.3> MomentumRNN: Integrating Momentum into Recurrent Neural Networks</font></a>. <i> Conference on Neural Information Processing Systems (NeurIPS), 2020</i>. 
<br />
<b>Tan M. Nguyen</b>, Richard G. Baraniuk, Andrea Bertozzi, Stanley J. Osher, Bao Wang.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2007.09200.pdf"><font color=blue size=+0.3> Neural Networks with Recurrent Generative Feedback</font></a>. <i> Conference on Neural Information Processing Systems (NeurIPS), 2020</i>.
<br />
Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, <b>Tan M. Nguyen</b>, Doris Tsao, Anima Anandkumar.</p>
</li>
</ul>
<ul>
<li><p><a href="nrm.pdf"><font color=blue size=+0.3> Neural Rendering Model: Joint Generation and Prediction for Semi-Supervised Learning</font></a>. <i>Deep Math Conference (DeepMath), 2019</i>. (Oral presentation)
<br />
<b>Tan M. Nguyen</b><font color=red size=-0.3><b>*</b></font>, Nhat Ho<font color=red size=-0.3><b>*</b></font>, Ankit B. Patel, Anima Anandkumar, Michael I. Jordan, Richard G. Baraniuk.</p>
</li>
</ul>
<ul>
<li><p><a href="cortically_inspired.pdf"><font color=blue size=+0.3> Towards a Cortically Inspired Deep Learning Model: Semi-Supervised Learning, Divisive Normalization, and Synaptic Pruning</font></a>. <i>Conference on Cognitive Computational Neuroscience (CCN), 2017</i>.
<br />
<b>Tan M. Nguyen</b>, Wanjia Liu, Fabian Sinz, Richard G. Baraniuk, Andreas S. Tolias, Xaq Pitkow, Ankit B. Patel.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1612.01936.pdf"><font color=blue size=+0.3> Probabilistic Framework for Deep Learning</font></a>. <i>Conference on Neural Information Processing Systems (NeurIPS), 2016</i>.
<br />
Ankit B. Patel, <b>Tan M. Nguyen</b>, Richard G Baraniuk.</p> 
</li>
</ul>
<h3 style="color:steelblue;"> Papers on Application</h3>
<ul>
<li><p><a href="https://arxiv.org/pdf/1911.05180.pdf"><font color=blue size=+0.3> Turbulence Forecasting via Neural ODE</font></a>. <i>NeurIPS Workshop on Machine Learning and the Physical Sciences, 2019</i>.
<br />
Gavin D. Portwood, Peetak P. Mitra, Mateus Dias Ribeiro, <b>Tan M. Nguyen</b>, Balasubramanya T. Nadiga, Juan A. Saenz, Michael Chertkov, Animesh Garg, Anima Anandkumar, Andreas Dengel, Richard G. Baraniuk, David P. Schmidt. </p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
<font color=red size=-0.3><b>*</b></font>: co-first authors.
<font color=red size=-0.3><b>**</b></font>: co-last authors.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
