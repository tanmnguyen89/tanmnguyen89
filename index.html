<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><br /></div>
<div class="menu-category"><b>Tan Minh Nguyen</b></div>
<hr>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<br>
<div class="menu-category"><b>Research</b></div>
<hr>
<div class="menu-item"><a href="publications_type.html">Publications&nbsp;(by&nbsp;type)</a></div>
<div class="menu-item"><a href="publications_topics.html">Publications&nbsp;(by&nbsp;topic)</a></div>
<div class="menu-item"><a href="collaborators.html">Students/&nbsp;Collaborators</a></div>
<div class="menu-item"><a href="workshops.html">Workshops/&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<h1 style="color:steelblue;">Tan Minh Nguyen</h1>
<table class="imgtable"><tr><td>
<img src="tan_profile.jpeg" alt="Coming Soon!" width="250px" height="250px" />&nbsp;</td>
<td align="left"><p>Post-doctoral Scholar<br />
<a href="https://ww3.math.ucla.edu/">Department of Mathematics</a> <br />
<a href="https://www.ucla.edu/">University of California, Los Angeles</a> <br /></p>
<p>Email: <a href="mailto:tanmnguyen89@ucla.edu">tanmnguyen89@ucla.edu</a> <br />
Office: Math Sciences 7310, 520 Portola Plaza, Los Angeles, CA 90095 </p>
</td></tr></table>
<h2 style="color:steelblue;">Brief Biography</h2>
<hr>
<p>I am currently a postdoctoral scholar in the Department of Mathematics at the University of California, Los Angeles, working with <a href="https://www.math.ucla.edu/~sjo/">Dr. Stanley J. Osher</a>. I have obtained my Ph.D. in Machine Learning from Rice University, where I was advised by <a href="https://richb.rice.edu/">Dr. Richard G. Baraniuk</a>. My research is focused on the intersection of Deep Learning, Probabilistic Modeling, Optimization, and ODEs/PDEs. I gave an invited talk in the Deep Learning Theory Workshop at NeurIPS 2018 and organized the 1st Workshop on Integration of Deep Neural Models and Differential Equations at ICLR 2020. I also had two awesome long internships with Amazon AI and NVIDIA Research, during which I worked with <a href="http://tensorlab.cms.caltech.edu/users/anima/">Dr. Anima Anandkumar</a>. I am the recipient of the prestigious <a href="https://cra.org/ccc/leadership-development/cifellows/">Computing Innovation Postdoctoral Fellowship (CIFellows)</a> from the Computing Research Association (CRA), the <a href="https://www.nsfgrfp.org/">NSF Graduate Research Fellowship</a>, and the <a href="http://www.igert.org/projects/301.html#:~:text=trainees%20are%E2%80%A6%20more%20%C2%BB-,This%20Integrative%20Graduate%20Education%20and%20Research%20Traineeship%20(IGERT)%20award%20provides,%2C%20mechanical%20engineering%2C%20and%20bioengineering.">IGERT Neuroengineering Traineeship</a>. I received my MSEE and BSEE from Rice in May 2018 and May 2014, respectively..</p>
<ul>
<p>I am also writing about simple ideas and principled approach that lead to working machine learning algorithms on my blog, <a href="http://almostconvergent.blogs.rice.edu/">Almost Convergent</a>.</p>
<h2 style="color:steelblue;">Research interests</h2>
<hr>
<p>My research focuses on developing models for scientific machine learning with applications in compu-
tational fluid dynamics (e.g. turbulence modeling) and numerical analysis (e.g. eﬀicient numerical
solvers) via three principled approaches:</p>
<ul>
<li><p>Optimization (momentum-based neural networks)</p>
</li>
<li><p>Differential equations  (graph neural diffusion, neural ordinary differential equations)</p>
</li>
<li><p>Statistical modeling  (deep generative models, mixture framework for transformers)</p>
</li>
</ul>
<h2 style="color:steelblue;"> Selected Publications on Theory</h2>
<hr style="margin-bottom:0cm;">
<ul>
<li><p><a href="https://arxiv.org/pdf/2005.11411.pdf"><font color=blue size=+0.3> Instability, computational efficiency, and statistical accuracy </font></a>. <i>Under revision</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p><a href="JMLR_V1.pdf"><font color=blue size=+0.3> Multivariate smoothing via the Fourier integral theorem and Fourier kernel </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="SDE_Arxiv_submission.pdf"><font color=blue size=+0.3> A diffusion process perspective on the posterior contraction rates for parameters</font></a>. <i>Under revision</i>.
<br />
Wenlong Mou<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="Bayesian_Consistency.pdf"><font color=blue size=+0.3> Bayesian consistency with the supremum metric </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="v_arxiv.pdf"<font color=blue size=+0.3> Beyond black box densities: Parameter learning for the deviated components</font></a>. <i>Under review</i>.
<br />
Dat Do<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Long Nguyen.</p>
</li>
</ul>
<ul>
<li><p><a href="Refined_metric.pdf"<font color=blue size=+0.3> Refined convergence rates for maximum likelihood estimation under finite mixture models</font></a>. <i> Under review</i>. 
<br />
Tudor Manole, <b>Nhat Ho</b>.
</li>
</ul>
<ul>
<li><p><a href="Polyak_Step_Size_V3.pdf"><font color=blue size=+0.3> Towards statistical and computational complexities of Polyak step size gradient descent </font></a>. <i>AISTATS, 2022</i>. 
<br />
Tongzheng Ren<font color=red size=-0.3><b>*</b></font>, Fuheng Cui<font color=red size=-0.3><b>*</b></font>, Alexia Atsidakou<font color=red size=-0.3><b>*</b></font>, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.02601.pdf"><font color=blue size=+0.3> On the minimax optimality of the EM algorithm for learning two-component mixed linear regression</font></a>. <i>AISTATS, 2021</i>. 
<br />
Jeong Y. Kwon, <b>Nhat Ho</b>, Constantine Caramanis.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1906.01437.pdf"><font color=blue size=+0.3> On the efficiency of the Sinkhorn and Greenkhorn algorithms and their acceleration for optimal transport</font></a>. <i>Under revision</i>. 
<br />
Tianyi Lin, <b>Nhat Ho</b>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="Mixture_Experts_Arxiv.pdf"><font color=blue size=+0.3> Convergence rates for Gaussian mixtures of experts</font></a>. <i>Under revision</i>.
<br />
<b>Nhat Ho</b>, Chiao-Yu Yang, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2102.02756.pdf"><font color=blue size=+0.3> On the computational and statistical complexity of over-parameterized matrix sensing </font></a>. <i>Under revision</i>.
<br />
Jiacheng Zhuo, Jeongyeol Kwon, <b>Nhat Ho</b>, Constantine Caramanis.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.07458.pdf"><font color=blue size=+0.3> Projection robust Wasserstein distance and Riemannian optimization </font></a>. <i> Advances in NeurIPS, 2020</i>. 
<br />
Tianyi Lin<font color=red size=-0.3><b>*</b></font>, Chenyou Fan<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>, Marco Cuturi, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p><a href="Normalized_GD.pdf"<font color=blue size=+0.3> Improving computational complexity in statistical models with second-order information </font></a>. <i> Under review</i>. 
<br />
Tongzheng Ren, Jiacheng Zhuo, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1901.05078.pdf"><font color=blue size=+0.3> On posterior contraction of parameters and interpretability in Bayesian mixture modeling </font></a>. <i> To appear, Bernoulli</i>.
<br />
Aritra Guha, <b>Nhat Ho</b>, XuanLong Nguyen.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1810.00828.pdf"><font color=blue size=+0.3> Singularity,
misspecification, and the convergence rate of EM</font></a>. <i>Annals of Statistics, 48(6), 3161-3182, 2020</i>. 
<br />
Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1609.02655.pdf"><font color=blue size=+0.3> Singularity structures and impacts on parameter estimation behavior in finite mixtures of distributions</font></a>. <i>SIAM Journal on Mathematics of Data Science (SIMODS), 1(4), 730–758, 2019</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen.</p>
</li>
</ul>
<ul>
<li><p><a href="AoS_2016.pdf"><font color=blue size=+0.3>Convergence rates of parameter estimation for some weakly identifiable finite mixtures</font></a>. <i>Annals of Statistics, 44(6), 2726-2755, 2016</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen</p>
</li>
</ul>
<h2 style="color:steelblue;"> Selected Publications on Method and Application (Generative Models, Optimal Transport, Transformer, Convolutional Neural Networks, etc.)</h2>
<hr style="margin-bottom:0cm;">
<ul>
<li><p><a href="https://arxiv.org/pdf/1811.02657.pdf"><font color=blue size=+0.3> A Bayesian perspective of convolutional neural networks through a deconvolutional generative model</font></a>. <i>Under revision</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Ankit Patel, Anima Anandkumar, Michael I. Jordan, Richard Baraniuk. </p>
</li>
</ul>
<ul>
<li><p><a href="Transformer_mixture_key.pdf"<font color=blue size=+0.3> Efficient Transformer with shared keys </font></a>. <i>Under review</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Dung Le, Khuong Nguyen, Anh Tran, Richard Baraniuk, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Stanley Osher<font color=red size=-0.3><b>*</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="Amortized_Projection_Optimization_for_Mini_batch_Projected_Wasserstein.pdf"><font color=blue size=+0.3> Amortized projection optimization for sliced Wasserstein generative models</font></a>. <i> Under review</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Revisiting projected Wasserstein metric on images</font></a>. <i> To be submitted</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Revisiting Transformer from a geometric perspective </font></a>. <i> To be submitted</i>. 
<br />
Tan Nguyen, Khai Nguyen, Richard Baraniuk, Stanley Osher<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="Statistical_Fourier_Theorem.pdf"><font color=blue size=+0.3> Statistical analysis using the Fourier integral theorem </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> An efficient method for sharing in Transformer </font></a>. <i> Under review</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Khai Nguyen, Hai Do, Richard Baraniuk, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>,Stanley Osher<font color=red size=-0.3><b>*</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.06448.pdf"><font color=blue size=+0.3> Probabilistic best subset selection via gradient-based optimization</font></a>. <i>Under review</i>. 
<br />
Mingzhang Yin, <b>Nhat Ho</b>, Bowei Yan, Xiaoning Qian, Mingyuan Zhou.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2020-08-19 03:32:40 CDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
